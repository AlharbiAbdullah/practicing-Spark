{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/05 22:21:14 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.8.100 instead (on interface wlp1s0)\n",
      "22/06/05 22:21:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/abdullah/spark/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/05 22:21:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/05 22:21:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('customers').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('customer_churn.csv' , inferSchema= True , header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------+-----+\n",
      "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|   Company|Churn|\n",
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------+-----+\n",
      "|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|Harvey LLC|    1|\n",
      "|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|Wilson PLC|    1|\n",
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the needed columns \n",
    "\n",
    "df_new = df.select([\n",
    "    'Age' , \n",
    "    'Total_Purchase' , \n",
    "    'Years' , \n",
    "    'Num_Sites' , \n",
    "    'Churn'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-----+---------+-----+\n",
      "| Age|Total_Purchase|Years|Num_Sites|Churn|\n",
      "+----+--------------+-----+---------+-----+\n",
      "|42.0|       11066.8| 7.22|      8.0|    1|\n",
      "+----+--------------+-----+---------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age', 'Total_Purchase', 'Years', 'Num_Sites', 'Churn']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----+---------+-----+\n",
      "|Age|Total_Purchase|Years|Num_Sites|Churn|\n",
      "+---+--------------+-----+---------+-----+\n",
      "|  0|             0|    0|        0|    0|\n",
      "+---+--------------+-----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count of nulls \n",
    "df_new.select([count(when(isnan(c), c)).alias(c) for c in df_new.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "assumbler = VectorAssembler(inputCols=[\n",
    "    'Age',\n",
    "    'Total_Purchase', \n",
    "    'Years', \n",
    "    'Num_Sites'\n",
    "] , outputCol= 'feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assumbler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = output.select('feature' , 'Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data \n",
    "\n",
    "train_data , test_data = selected_df.randomSplit([0.7 , 0.3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model = LogisticRegression(featuresCol= 'feature' ,\n",
    "                                     labelCol= 'Churn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a pipeline \n",
    "\n",
    "pipeline  = Pipeline(stages=[\n",
    "    assumbler , \n",
    "    output , \n",
    "    log_reg_model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot recognize a pipeline stage of type <class 'pyspark.sql.dataframe.DataFrame'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/abdullah/onmytable/spark_study/spark_udemy_course/ml_spark/logistic_regression/customers_logistic_regression.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abdullah/onmytable/spark_study/spark_udemy_course/ml_spark/logistic_regression/customers_logistic_regression.ipynb#ch0000016?line=0'>1</a>\u001b[0m \u001b[39m# fit the model \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/abdullah/onmytable/spark_study/spark_udemy_course/ml_spark/logistic_regression/customers_logistic_regression.ipynb#ch0000016?line=1'>2</a>\u001b[0m fit_model \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mfit(train_data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/base.py?line=158'>159</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/base.py?line=159'>160</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/base.py?line=160'>161</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/base.py?line=161'>162</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/base.py?line=162'>163</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/base.py?line=163'>164</a>\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py:101\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py?line=98'>99</a>\u001b[0m \u001b[39mfor\u001b[39;00m stage \u001b[39min\u001b[39;00m stages:\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py?line=99'>100</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(stage, Estimator) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(stage, Transformer)):\n\u001b[0;32m--> <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py?line=100'>101</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py?line=101'>102</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot recognize a pipeline stage of type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(stage))\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py?line=102'>103</a>\u001b[0m indexOfLastEstimator \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py?line=103'>104</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, stage \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(stages):\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot recognize a pipeline stage of type <class 'pyspark.sql.dataframe.DataFrame'>."
     ]
    }
   ],
   "source": [
    "# fit the model \n",
    "fit_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform \n",
    "\n",
    "results = fit_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='prediction' , \n",
    "    labelCol='Churn'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area unver the curve \n",
    "\n",
    "AUC = evaluation.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7607516614467954"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a new customers data to test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "feature does not exist. Available: Names, Age, Total_Purchase, Account_Manager, Years, Num_Sites, Onboard_date, Location, Company, Churn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m/home/abdullah/onmytable/spark_study/spark_udemy_course/ml_spark/logistic_regression/customers_logistic_regression.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abdullah/onmytable/spark_study/spark_udemy_course/ml_spark/logistic_regression/customers_logistic_regression.ipynb#ch0000029?line=0'>1</a>\u001b[0m \u001b[39m# apply the model to all of the data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/abdullah/onmytable/spark_study/spark_udemy_course/ml_spark/logistic_regression/customers_logistic_regression.ipynb#ch0000029?line=2'>3</a>\u001b[0m final_model \u001b[39m=\u001b[39m log_reg_model\u001b[39m.\u001b[39;49mfit(df)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/base.py?line=158'>159</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/base.py?line=159'>160</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/base.py?line=160'>161</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/base.py?line=161'>162</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/base.py?line=162'>163</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/base.py?line=163'>164</a>\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py?line=333'>334</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset):\n\u001b[0;32m--> <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py?line=334'>335</a>\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py?line=335'>336</a>\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py?line=336'>337</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py?line=317'>318</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py?line=318'>319</a>\u001b[0m \u001b[39mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py?line=319'>320</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py?line=328'>329</a>\u001b[0m \u001b[39m    fitted Java model\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py?line=329'>330</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py?line=330'>331</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py?line=331'>332</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/py4j/java_gateway.py?line=1314'>1315</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/py4j/java_gateway.py?line=1319'>1320</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/py4j/java_gateway.py?line=1323'>1324</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/py4j/java_gateway.py?line=1324'>1325</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/sql/utils.py?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/sql/utils.py?line=114'>115</a>\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/sql/utils.py?line=115'>116</a>\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/sql/utils.py?line=116'>117</a>\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/sql/utils.py?line=117'>118</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/abdullah/.local/lib/python3.10/site-packages/pyspark/sql/utils.py?line=118'>119</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: feature does not exist. Available: Names, Age, Total_Purchase, Account_Manager, Years, Num_Sites, Onboard_date, Location, Company, Churn"
     ]
    }
   ],
   "source": [
    "# apply the model to all of the data\n",
    "\n",
    "final_model = log_reg_model.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_customers = spark.read.csv('new_customers.csv' , inferSchema= True , \n",
    "                                    header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------+\n",
      "|        Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location| Company|\n",
      "+-------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------+\n",
      "|Andrew Mccall|37.0|       9935.53|              1| 7.71|      8.0|2011-08-29 18:37:54|38612 Johnny Stra...|King Ltd|\n",
      "+-------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_customers.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_new_customers = assumbler.transform(new_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- feature: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing_new_customers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'LogisticRegression' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/abdullah/onmytable/spark_study/spark_udemy_course/ml_spark/logistic_regression/customers_logistic_regression.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/abdullah/onmytable/spark_study/spark_udemy_course/ml_spark/logistic_regression/customers_logistic_regression.ipynb#ch0000027?line=0'>1</a>\u001b[0m new_customers_result \u001b[39m=\u001b[39m log_reg_model(testing_new_customers)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'LogisticRegression' object is not callable"
     ]
    }
   ],
   "source": [
    "new_customers_result = log_reg_model(testing_new_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
