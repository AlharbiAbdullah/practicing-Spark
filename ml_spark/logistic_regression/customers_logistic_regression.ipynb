{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('customers').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('customer_churn.csv' , inferSchema= True , header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------+-----+\n",
      "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|   Company|Churn|\n",
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------+-----+\n",
      "|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|Harvey LLC|    1|\n",
      "|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|Wilson PLC|    1|\n",
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the needed columns \n",
    "\n",
    "df_new = df.select([\n",
    "    'Age' , \n",
    "    'Total_Purchase' , \n",
    "    'Years' , \n",
    "    'Num_Sites' , \n",
    "    'Churn'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-----+---------+-----+\n",
      "| Age|Total_Purchase|Years|Num_Sites|Churn|\n",
      "+----+--------------+-----+---------+-----+\n",
      "|42.0|       11066.8| 7.22|      8.0|    1|\n",
      "+----+--------------+-----+---------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age', 'Total_Purchase', 'Years', 'Num_Sites', 'Churn']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----+---------+-----+\n",
      "|Age|Total_Purchase|Years|Num_Sites|Churn|\n",
      "+---+--------------+-----+---------+-----+\n",
      "|  0|             0|    0|        0|    0|\n",
      "+---+--------------+-----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count of nulls \n",
    "df_new.select([count(when(isnan(c), c)).alias(c) for c in df_new.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "assumbler = VectorAssembler(inputCols=[\n",
    "    'Age',\n",
    "    'Total_Purchase', \n",
    "    'Years', \n",
    "    'Num_Sites'\n",
    "] , outputCol= 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assumbler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output.select('features' , 'Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data \n",
    "\n",
    "train_data , test_data = final_data.randomSplit([0.7 , 0.3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_churn = LogisticRegression( featuresCol='features' , \n",
    "                                     labelCol= 'Churn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_churn_model = lr_churn.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show summary \n",
    "\n",
    "training_summary = fitted_churn_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdullah/.local/lib/python3.10/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|             Churn|        prediction|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               628|               628|\n",
      "|   mean|0.1751592356687898|0.1321656050955414|\n",
      "| stddev|0.3804062381494544|0.3389406867257665|\n",
      "|    min|               0.0|               0.0|\n",
      "|    max|               1.0|               1.0|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_summary.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_and_labels = fitted_churn_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|Churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[26.0,8787.39,5.4...|    1|[0.76972650037697...|[0.68346172730619...|       0.0|\n",
      "|[27.0,8628.8,5.3,...|    0|[5.28218130862694...|[0.99494435469028...|       0.0|\n",
      "|[28.0,11204.23,3....|    0|[1.47160163497859...|[0.81330070460468...|       0.0|\n",
      "|[29.0,5900.78,5.5...|    0|[4.16612223056924...|[0.98472465886991...|       0.0|\n",
      "|[29.0,12711.15,5....|    0|[4.54980268433604...|[0.98954125187394...|       0.0|\n",
      "|[30.0,7960.64,2.7...|    1|[3.34871518275153...|[0.96606273764776...|       0.0|\n",
      "|[30.0,12788.37,4....|    0|[1.97257162561516...|[0.87788706297148...|       0.0|\n",
      "|[30.0,13473.35,3....|    0|[2.18392051969072...|[0.89879624664540...|       0.0|\n",
      "|[31.0,5304.6,5.29...|    0|[3.16260075892582...|[0.95940236571893...|       0.0|\n",
      "|[31.0,7073.61,5.7...|    0|[2.74171080981795...|[0.93944349685287...|       0.0|\n",
      "|[31.0,8688.21,3.5...|    0|[6.08723605174579...|[0.99773346978021...|       0.0|\n",
      "|[31.0,8829.83,4.5...|    0|[4.40557878888780...|[0.98793822475726...|       0.0|\n",
      "|[31.0,9574.89,7.3...|    0|[2.68823904565569...|[0.93632907959469...|       0.0|\n",
      "|[31.0,11297.57,6....|    1|[0.93949544553225...|[0.71899772849921...|       0.0|\n",
      "|[31.0,12264.68,5....|    0|[3.29276748914002...|[0.96417985793555...|       0.0|\n",
      "|[32.0,8011.38,5.3...|    0|[1.70925935158898...|[0.84674019405538...|       0.0|\n",
      "|[32.0,8575.71,4.2...|    0|[3.41327299281726...|[0.96811678385546...|       0.0|\n",
      "|[32.0,9472.72,1.0...|    0|[4.11677512753063...|[0.98396434730913...|       0.0|\n",
      "|[32.0,11715.72,4....|    0|[2.84798297766399...|[0.94521432663719...|       0.0|\n",
      "|[32.0,12142.99,5....|    0|[4.88294972130169...|[0.99248230584037...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_and_labels.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_eval = BinaryClassificationEvaluator(rawPredictionCol= 'prediction' ,\n",
    "                                            labelCol= 'Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area Under the Curve \n",
    "\n",
    "AUC = churn_eval.evaluate(pred_and_labels.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7202586206896552"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mimic applying the model to new data \n",
    "\n",
    "# First I need to apply the model to the whole dataset \n",
    "\n",
    "final_lr_model = lr_churn.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_customers = spark.read.csv('new_customers.csv' , inferSchema=True ,header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# making sure it's same data & format \n",
    "new_customers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_customers = assumbler.transform(new_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = final_lr_model.transform(test_new_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|         Company|prediction|\n",
      "+----------------+----------+\n",
      "|        King Ltd|       0.0|\n",
      "|   Cannon-Benson|       1.0|\n",
      "|Barron-Robertson|       1.0|\n",
      "|   Sexton-Golden|       1.0|\n",
      "|        Wood LLC|       0.0|\n",
      "|   Parks-Robbins|       1.0|\n",
      "+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_result.select('Company' , 'prediction').show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
